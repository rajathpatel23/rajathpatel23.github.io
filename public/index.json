[{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}\\pi\\left[\\sum{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}\\pi\\left[\\sum{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\\\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\\\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if } \\text{done}i \\\\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\left{ \\begin{array}{ll} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{array} \\right.$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\left{ \\begin{array}{ll} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{array} \\right.$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\left{ \\begin{array}{ll} r_i \u0026amp; \\text{if } \\text{done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{array} \\right.$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = \\begin{cases} r_i \u0026amp; \\text{if done}i \\ r_i + \\gamma \\max{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \u0026amp; \\text{otherwise} \\end{cases}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\arg\\max{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\operatorname*{argmax}{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\operatorname*{argmax}{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\operatorname*{argmax}{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}\\left(s\u0026rsquo;i, \\operatorname*{argmax}{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)\\right)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;i, a^) \\quad \\text{where } a^ = \\underset{a\u0026rsquo;}{\\arg\\max} , Q\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;i, a^) \\quad \\text{where } a^ = \\underset{a\u0026rsquo;}{\\arg\\max} , Q\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;i, a^) \\quad \\text{where } a^ = \\underset{a\u0026rsquo;}{\\arg\\max} , Q\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}{a\u0026rsquo;} Q\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}{a\u0026rsquo;} Q\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}{a\u0026rsquo;} Q\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}{a\u0026rsquo;} Q\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"}]