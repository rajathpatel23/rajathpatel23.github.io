[{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003ch1 id=\"attention-mechanisms-in-transformers-mha-vs-mqa-vs-gqa\"\u003eAttention Mechanisms in Transformers: MHA vs MQA vs GQA\u003c/h1\u003e\n\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":""},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"},{"content":"This guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\nQuick Overview Self-Attention: Each token looks at other tokens to build contextualized representations Multi-Head Attention (MHA): Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections Multi-Query Attention (MQA): Share Key/Value projections across all query heads; reduces parameters significantly Grouped-Query Attention (GQA): Groups of query heads share K/V projections; balances expressiveness and efficiency 1. Self-Attention Foundations Core Intuition Self-attention is a content-based lookup over the sequence:\nQueries (Q) ask \u0026ldquo;what do I need?\u0026rdquo; Keys (K) say \u0026ldquo;what do I offer?\u0026rdquo; Values (V) contain the information to aggregate Core computation: softmax((Q @ K^T) / sqrt(d)) @ V Mathematical Formulation Notation: batch B, sequence length L, model width D\nSingle-head projections:\nQ = X W_q, K = X W_k, V = X W_v Shapes: X: [B, L, D], W_q/W_k/W_v: [D, d], Q/K/V: [B, L, d] Attention computation:\n1. Attention logits: A = (Q @ K^T) / sqrt(d) → [B, L, L] 2. Apply causal mask: A[i,j] = -inf for j \u0026gt; i (generation) 3. Attention weights: P = softmax(A, dim=-1) 4. Output: Y = P @ V → [B, L, d] Implementation def scaled_dot_product_attention(Q, K, V, mask=None): # Q,K,V: [B, L, d] scale = 1.0 / math.sqrt(Q.size(-1)) logits = torch.einsum(\u0026#39;bld,bmd-\u0026gt;blm\u0026#39;, Q, K) * scale # [B,L,L] if mask is not None: logits = logits.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) probs = torch.softmax(logits, dim=-1) return torch.einsum(\u0026#39;blm,bmd-\u0026gt;bld\u0026#39;, probs, V) Limitations A single head blends all relations into one pattern, which can be too coarse for complex linguistic structure.\n2. Multi-Head Attention (MHA) - The Gold Standard Why Multiple Heads? Different heads specialize in different aspects:\nSyntactic patterns: Subject-verb relationships, phrase boundaries Long-range dependencies: Coreference resolution, discourse structure Entity tracking: Following entities across long sequences Semantic relationships: Similarity, causation, temporal ordering Architecture Details Head configuration:\nModel width D, number of heads H, head width d_h = D / H Combined projections: W_q/W_k/W_v: [D, H×d_h] After projection, reshape to heads: Q/K/V: [B, H, L, d_h] Processing flow:\n# Multi-head projection Q = X @ W_q; K = X @ W_k; V = X @ W_v # [B,L,D] Q = Q.view(B, L, H, d_h).transpose(1, 2) # [B,H,L,d_h] # ... attention per head ... Y = Y_heads.transpose(1, 2).contiguous().view(B, L, D) @ W_o Computational Cost Attention compute: O(B × H × L² × d_h) Memory for logits: O(B × H × L²) Typical settings: D=4096, H=32, d_h=128 (many 7-70B models use H ∈ [24,40]) Trade-offs ✅ Advantages:\nHighest model quality and representational richness Each head can specialize in different linguistic phenomena Well-established training dynamics ❌ Disadvantages:\nKV cache scales with num_heads, increasing inference memory Higher bandwidth requirements during generation More expensive for long-context applications 3. Multi-Query Attention (MQA) - Maximum Efficiency Core Innovation Share Key and Value projections across all query heads while keeping query heads independent.\nWhy This Works Parameter efficiency: Dramatically reduces the number of parameters in attention layers Key insight: Queries can be diverse, but Keys/Values can be shared across heads Result: Significant parameter reduction with minimal quality loss in many tasks Architecture Changes Projection differences:\n# MHA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_model) # H heads worth self.w_v = nn.Linear(d_model, d_model) # H heads worth # MQA projections self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, d_k) # 1 head only (shared) self.w_v = nn.Linear(d_model, d_k) # 1 head only (shared) Runtime sharing:\nProjection weights: One W_k: [D, d_h] and W_v: [D, d_h] for all heads Runtime tensors: Single K: [B, L, d_h] and V: [B, L, d_h] referenced by every query head Queries remain multi-headed: Q: [B, H, L, d_h] for diverse attention patterns Attention Pattern Differences What changes in MQA:\nQuery diversity maintained: Each head still has independent query patterns Shared key/value space: All heads attend over the same key-value representations Reduced expressiveness: Some loss in ability to learn specialized key-value transformations per head When to Use MQA ✅ Advantages:\nSignificant parameter reduction (46.9% in our experiments) Faster training due to fewer parameters Maintains most attention expressiveness through diverse queries ⚠️ Trade-offs:\nSlightly lower quality than MHA on some complex tasks Less specialized key-value transformations per head May require careful hyperparameter tuning Real-World Usage PaLM: Uses MQA for efficiency in large-scale deployment Falcon: Adopted MQA for fast inference Chinchilla: Demonstrated MQA effectiveness at scale 4. Grouped-Query Attention (GQA) - The Sweet Spot Design Philosophy Compromise between MHA quality and MQA efficiency by dividing heads into groups that share K/V.\nArchitecture Details Group organization:\nDivide H heads into G groups Each group shares one K/V set: num_kv_heads = G Queries per group: H / G Projection structure:\n# GQA projections (G groups) self.w_q = nn.Linear(d_model, d_model) # H heads worth self.w_k = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth self.w_v = nn.Linear(d_model, num_kv_heads * d_k) # G heads worth Runtime organization:\nPer group: One W_k^g: [D, d_h] and W_v^g: [D, d_h] Runtime tensors per group: K^g: [B, L, d_h], V^g: [B, L, d_h] Head mapping: Query head index → group index for K/V routing Parameter Scaling Memory and computation:\nParameters scale with: num_kv_heads instead of num_heads Example (D=4096, H=32, d_h=128, G=4): GQA parameters: Proportional to 4 KV heads instead of 32 Reduction: ~8× fewer K/V parameters than MHA Performance Characteristics Quality vs. Efficiency:\nvs MHA: 90-95% of quality with major memory savings vs MQA: Better quality with moderate memory increase Sweet spot: G ∈ {4,8} groups work well empirically Implementation Considerations # Head-to-group mapping def get_kv_group(head_idx, num_heads, num_groups): return head_idx // (num_heads // num_groups) # Attention routing for head in range(num_heads): group = get_kv_group(head, num_heads, num_groups) attention_output[head] = attention(Q[head], K[group], V[group]) Real-World Adoption Llama-2: Uses GQA with optimized group configurations Code Llama: Balances code understanding with efficiency Mistral: Adopted GQA for production deployments 5. Practical Implementation Results Experimental Setup Our implementation demonstrates these mechanisms using Shakespeare\u0026rsquo;s text:\nInput: \u0026ldquo;To be or not to be, that is the question\u0026rdquo; Model dimensions: d_model=256, num_heads=16, num_kv_heads=4 (for GQA) Framework: PyTorch with custom implementations Parameter Comparison 📊 Attention Mechanisms Efficiency Analysis Model dimension (d_model): 256, Number of heads: 16 Multi-Head Attention (MHA): 263,168 parameters Multi-Query Attention (MQA): 139,808 parameters (46.9% reduction) Grouped-Query Attention (GQA): 164,480 parameters (37.5% reduction) Parameter Breakdown MHA (Traditional):\nQ projection: 256 × 256 = 65,536 parameters K projection: 256 × 256 = 65,536 parameters V projection: 256 × 256 = 65,536 parameters Output projection: 256 × 256 = 65,536 parameters Total: ~262K parameters MQA (Maximum Efficiency):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 16 = 4,096 parameters (1 shared head) V projection: 256 × 16 = 4,096 parameters (1 shared head) Output projection: 256 × 256 = 65,536 parameters Total: ~139K parameters (46.9% reduction) GQA (Balanced):\nQ projection: 256 × 256 = 65,536 parameters (16 heads) K projection: 256 × 64 = 16,384 parameters (4 KV heads) V projection: 256 × 64 = 16,384 parameters (4 KV heads) Output projection: 256 × 256 = 65,536 parameters Total: ~164K parameters (37.5% reduction) 6. Selection Guide: When to Use Each Mechanism Decision Matrix Criterion MHA GQA MQA Model Quality Highest High Good Parameter Efficiency Lowest Good Best Training Speed Slowest Fast Fastest Attention Expressiveness Maximum High Limited Implementation Simplicity Simple Moderate Simple Concrete Recommendations Choose MHA when:\nMaximum model quality is required Parameter count is not a constraint Research/experimentation phase You need maximum attention expressiveness Choose GQA when:\nNeed balance between quality and efficiency Building production systems with parameter constraints Want to reduce model size while maintaining good performance Quality-sensitive tasks with efficiency requirements Choose MQA when:\nParameter efficiency is critical Training on limited computational resources Building lightweight models for deployment Maximum parameter reduction is needed Model Size Considerations Large models (\u0026gt;10B parameters): Parameter efficiency becomes more important, GQA/MQA more attractive Medium models (1B-10B): GQA often provides the best balance Small models (\u0026lt;1B): MHA may be preferred for maximum quality with manageable parameter count 7. Key Implementation Insights Attention Pattern Differences How each mechanism processes information:\nMHA (Multi-Head Attention):\nEach head learns completely independent attention patterns Maximum expressiveness: heads can specialize in different linguistic phenomena Each head has its own \u0026ldquo;view\u0026rdquo; of what\u0026rsquo;s important in the sequence MQA (Multi-Query Attention):\nAll heads share the same Key/Value representations Query heads maintain diversity in what they \u0026ldquo;ask for\u0026rdquo; Reduced specialization in how information is represented (shared K/V) GQA (Grouped-Query Attention):\nGroups of heads share Key/Value representations Balances specialization (within groups) with efficiency (shared K/V) Creates \u0026ldquo;clusters\u0026rdquo; of heads that work with similar information representations Common Implementation Pitfalls ⚠️ Watch out for:\nForgetting sqrt(d_h) scaling in attention computation Wrong head→group mapping in GQA implementation Inconsistent tensor shapes when switching between mechanisms Not properly reshaping tensors for multi-head computation Conclusion The evolution from MHA → GQA → MQA represents a fundamental trade-off in attention mechanisms: expressiveness vs. efficiency.\nOur Experimental Results Show: Parameter Efficiency: MQA achieves 46.9% reduction, GQA achieves 37.5% reduction Quality Trade-offs: GQA maintains ~90-95% of MHA quality with major efficiency gains Practical Impact: The choice of attention mechanism significantly affects model size and training efficiency Key Takeaway Understanding these attention variants is essential for modern transformer development. The \u0026ldquo;best\u0026rdquo; choice depends on your specific constraints:\nResearch/Maximum Quality: MHA Production Balance: GQA Resource Constraints: MQA Each mechanism represents a different point on the quality-efficiency spectrum, and the field continues to find new ways to optimize this fundamental trade-off in transformer architectures.\n","permalink":"http://localhost:1313/posts/attention_mechanism/","summary":"\u003cp\u003eThis guide explores the core attention variants in modern transformers, focusing on the mechanisms themselves: Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). We\u0026rsquo;ll understand why each exists and their fundamental architectural differences.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"quick-overview\"\u003eQuick Overview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention\u003c/strong\u003e: Each token looks at other tokens to build contextualized representations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Head Attention (MHA)\u003c/strong\u003e: Multiple independent attention \u0026ldquo;heads\u0026rdquo; in parallel; each head has its own Q, K, V projections\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Query Attention (MQA)\u003c/strong\u003e: Share Key/Value projections across all query heads; reduces parameters significantly\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGrouped-Query Attention (GQA)\u003c/strong\u003e: Groups of query heads share K/V projections; balances expressiveness and efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-self-attention-foundations\"\u003e1. Self-Attention Foundations\u003c/h2\u003e\n\u003ch3 id=\"core-intuition\"\u003eCore Intuition\u003c/h3\u003e\n\u003cp\u003eSelf-attention is a \u003cstrong\u003econtent-based lookup\u003c/strong\u003e over the sequence:\u003c/p\u003e","title":"Attention Mechanisms in Transformers: MHA vs MQA vs GQA"},{"content":"Introduction Language models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\n1. Naive Approaches and Their Flaws Word-Level Tokenization The most intuitive approach: split text by spaces and punctuation.\nProblems: Vocabulary Explosion: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive. Out-of-Vocabulary (OOV) Words: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u0026lt;UNK\u0026gt; (unknown) token, losing all semantic meaning. Poor Generalization: The model treats eat, eating, and eaten as three completely separate, unrelated tokens. It fails to capture the shared root eat, making it harder to learn morphological relationships. Character-Level Tokenization The opposite extreme: split text into individual characters.\nAdvantage: The vocabulary is tiny and fixed (e.g., ~256 for bytes). There are no OOV words. Problems: Loss of Semantic Meaning: A single character like t has very little semantic value on its own. The model must expend significant capacity just to learn the structure of common words from characters. Long Sequences: A simple sentence becomes a very long sequence of tokens. This makes it computationally difficult for the model to learn long-range dependencies and can quickly exceed the context window. 2. The Solution: Subword Tokenization Modern LLMs use a hybrid approach called subword tokenization, which combines the best of both worlds.\nCore Idea: Common words are kept as single, unique tokens (e.g., the, is, and), while rare or complex words are broken down into smaller, meaningful sub-units (e.g., tokenization -\u0026gt; token + ization). Benefits: It gracefully handles OOV words by breaking them into known subwords. It captures morphological relationships (e.g., eating -\u0026gt; eat + ing). It keeps the vocabulary size manageable while maintaining semantic richness. Algorithm: Byte-Pair Encoding (BPE) BPE is a data-driven algorithm used to create a subword vocabulary. It starts with a base vocabulary of individual characters (or bytes) and learns merge rules from a large text corpus.\nInitialization: The initial vocabulary consists of all single bytes (0-255). Iteration: a. Count the frequency of all adjacent token pairs in the training corpus. b. Find the most frequent pair (e.g., ('e', 'r')). c. Merge this pair into a new, single token ('er'). d. Add this new token to the vocabulary and replace all occurrences of the pair in the corpus with the new token. Repeat: Continue this process for a predetermined number of merges, which defines the final vocabulary size. The byte_pair_encoding_algo.py script in this repository provides a simple, hands-on example of this process.\nBPE Training Process Visualization Our implementation shows the BPE training process in real-time:\nPre-processing: The tokenizer processes ~1MB of Shakespeare text Word Tokenization: Identifies 15,057 unique word tokens Pair Counting: Analyzes frequency of adjacent character/token pairs Merge Computation: Learns 12,455 merge rules to build the final vocabulary This creates a vocabulary that efficiently represents Shakespeare\u0026rsquo;s linguistic patterns, learning common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;to\u0026rdquo; as single tokens while breaking down rare words into meaningful subparts.\n3. Modern Tokenizers in GPT Models OpenAI\u0026rsquo;s models use a library called tiktoken for high-performance BPE tokenization.\ncl100k_base: The tokenizer used for models like GPT-3.5 and GPT-4. It has a vocabulary size of ~100,000. o200k_base: A newer tokenizer for more advanced models. It has a larger vocabulary of ~200,000 and includes special tokens designed for chat and instruction-following. Special Tokens for Chat Modern tokenizers reserve special tokens to encode conversational structure, which is critical for chat models.\nChat Formatting: Tokens like \u0026lt;|im_start|\u0026gt; and \u0026lt;|im_end|\u0026gt; are used to delineate turns in a conversation, allowing the model to understand the flow of a multi-speaker dialogue. Role-Based Prompting: Tokens are used to specify roles (e.g., system, user, assistant), preserving the instructions and context for how the model should behave. These special tokens are not part of the regular text but are essential metadata that the model uses to understand its task. As shown in tokenizer.py, you can enable them with allowed_special=\u0026quot;all\u0026quot;.\nComprehensive Tokenization Comparison Let\u0026rsquo;s compare different tokenization approaches with practical examples:\nExample 1: Simple Text Input: \u0026quot;Hello world!\u0026quot;\nCharacter-level: 12 tokens ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'] Word-level: 2 tokens ['Hello', 'world!'] GPT-4 BPE: 3 tokens ['Hello', ' world', '!'] Custom BPE: 4 tokens ['H', 'ello', ' world', '!'] Example 2: Complex Words Input: \u0026quot;Supercalifragilisticexpialidocious\u0026quot;\nCharacter-level: 34 tokens (one per character) Word-level: 1 token (entire word, likely OOV) GPT-4 BPE: 11 tokens ['Sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious'] This demonstrates BPE\u0026rsquo;s key advantage: it gracefully handles unknown words by breaking them into meaningful subparts.\nExample 3: Modern Terms Input: \u0026quot;COVID-19 vaccination appointments\u0026quot;\nGPT-4 BPE: 5 tokens ['COVID', '-', '19', ' vaccination', ' appointments'] Custom Shakespeare BPE: 10 tokens (struggles with modern terms) This shows how tokenizer training data affects performance on different domains.\nBPE Benefits Analysis Morphological Understanding \u0026quot;running\u0026quot; → 1 token (common word, learned as whole) \u0026quot;unhappiness\u0026quot; → 3 tokens ['un', 'h', 'appiness'] (breaks down prefix/suffix) \u0026quot;tokenization\u0026quot; → 2 tokens ['token', 'ization'] (recognizes root + suffix) Compression Efficiency From our analysis of a technical paragraph:\nCharacter-level: 344 tokens (1:1 ratio) Word-level: 49 tokens (7x compression) BPE: 67 tokens (5.1x compression vs characters, better vocabulary management) Key Insights Vocabulary Size: BPE requires much smaller vocabulary than word-level OOV Handling: Never encounters truly unknown tokens Compression: Balances sequence length with vocabulary size Domain Adaptation: Learns patterns specific to training data Practical Demonstration: Shakespeare Tokenizer in Action Now that we understand the theory, let\u0026rsquo;s see tokenization in action using our custom BPE tokenizer trained on Shakespeare\u0026rsquo;s complete works:\nShakespeare-Optimized Tokenization Our custom tokenizer shows interesting domain-specific behavior:\nExample 1: Classic Shakespeare\nInput: \u0026#34;To be or not to be, that is the question\u0026#34; Tokens: [\u0026#39;To\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;Ġor\u0026#39;, \u0026#39;Ġnot\u0026#39;, \u0026#39;Ġto\u0026#39;, \u0026#39;Ġbe\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġthat\u0026#39;, \u0026#39;Ġis\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġquestion\u0026#39;] Token Count: 11 tokens Example 2: Shakespearean Language\nInput: \u0026#34;Thou art more lovely and more temperate\u0026#34; Tokens: [\u0026#39;Thou\u0026#39;, \u0026#39;Ġart\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġlovely\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġmore\u0026#39;, \u0026#39;Ġtemperate\u0026#39;] Token Count: 7 tokens Example 3: Famous Shakespeare Quote\nInput: \u0026#34;All the world\u0026#39;s a stage, and all the men and women merely players\u0026#34; Tokens: [\u0026#39;All\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġworld\u0026#39;, \u0026#34;\u0026#39;s\u0026#34;, \u0026#39;Ġa\u0026#39;, \u0026#39;Ġstage\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġall\u0026#39;, \u0026#39;Ġthe\u0026#39;, \u0026#39;Ġmen\u0026#39;, \u0026#39;Ġand\u0026#39;, \u0026#39;Ġwomen\u0026#39;, \u0026#39;Ġmerely\u0026#39;, \u0026#39;Ġplayers\u0026#39;] Token Count: 15 tokens Implementation Code Our tokenizer training implementation demonstrates the BPE process:\ndef train_tokenizer(data_path: str, tokenizer_path: str): # Initialize BPE tokenizer tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False) tokenizer.post_processor = ByteLevelPostProcessor(trim_offsets=False) tokenizer.decoder = decoder.ByteLevel() # Train with special tokens and minimum frequency trainer = BpeTrainer(special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;], min_frequency=2) tokenizer.train([data_path], trainer) return tokenizer Analysis of Results Domain Adaptation Success:\nArchaic Words: \u0026ldquo;Thou\u0026rdquo;, \u0026ldquo;art\u0026rdquo; are learned as single tokens (common in Shakespeare) Complex Words: \u0026ldquo;temperate\u0026rdquo; stays whole, showing efficient learning from corpus Modern Efficiency: Our tokenizer has ~12,521 tokens, optimized for Early Modern English Technical Insights:\nSpace Encoding: The \u0026lsquo;Ġ\u0026rsquo; prefix indicates spaces (ByteLevel encoding standard) Punctuation Handling: Contractions like \u0026ldquo;\u0026rsquo;s\u0026rdquo; and punctuation are separate tokens Compression: Achieves good balance between sequence length and vocabulary size This demonstrates how BPE tokenizers adapt to their training domain, making them highly effective for specific text types while maintaining the flexibility to handle any input text through subword decomposition.\n","permalink":"http://localhost:1313/posts/tokenizers/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLanguage models are mathematical functions; they operate on numbers, not raw text. Tokenization is the crucial first step in converting human-readable text into a sequence of integers (tokens) that a model can process. These tokens are then mapped to embedding vectors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"1-naive-approaches-and-their-flaws\"\u003e1. Naive Approaches and Their Flaws\u003c/h3\u003e\n\u003ch4 id=\"word-level-tokenization\"\u003eWord-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe most intuitive approach: split text by spaces and punctuation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProblems\u003c/strong\u003e:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Explosion\u003c/strong\u003e: A language like English has hundreds of thousands of words. The model\u0026rsquo;s vocabulary would be enormous, making the final embedding and output layers computationally massive.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOut-of-Vocabulary (OOV) Words\u003c/strong\u003e: If the model encounters a word not seen during training (e.g., a new slang term, a typo, or a technical name), it has no token for it. It typically maps it to an \u003ccode\u003e\u0026lt;UNK\u0026gt;\u003c/code\u003e (unknown) token, losing all semantic meaning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoor Generalization\u003c/strong\u003e: The model treats \u003ccode\u003eeat\u003c/code\u003e, \u003ccode\u003eeating\u003c/code\u003e, and \u003ccode\u003eeaten\u003c/code\u003e as three completely separate, unrelated tokens. It fails to capture the shared root \u003ccode\u003eeat\u003c/code\u003e, making it harder to learn morphological relationships.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"character-level-tokenization\"\u003eCharacter-Level Tokenization\u003c/h4\u003e\n\u003cp\u003eThe opposite extreme: split text into individual characters.\u003c/p\u003e","title":"Understanding Tokenization: From Text to Integers"},{"content":"Introduction Reinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\nMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand how they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\nThis post is the first in a series where we\u0026rsquo;ll start with foundations:\nWhat is an MDP and how does it differ from an HMM? How do rewards and value functions actually work? How can we move from knowing state values to learning optimal actions with Q-learning? And how do we scale that to Deep Q-Learning when the state space explodes? By the end, you\u0026rsquo;ll have a baseline RL toolkit — from mathematical definitions to runnable code — and a clear picture of how these pieces fit together when building agents.\nWhy This Series? When I first encountered MDPs, I assumed they were simply an extension of Hidden Markov Models. I quickly learned this was wrong. HMMs deal with hidden states and observations, while MDPs deal with fully observable states and decision-making under uncertainty. Understanding this difference changed how I approached RL problems.\nThis blog is written in a learning-by-doing style — meaning you\u0026rsquo;ll see the concepts, math, and code side by side, along with real-world analogies (including fraud detection examples) and small \u0026ldquo;try-it-yourself\u0026rdquo; prompts.\nPart 1: Foundations with MDPs (and a \u0026ldquo;HMM aside\u0026rdquo;) Markov Decision Process (MDP) Markov Decision Process (MDP) models an agent interacting with an environment over discrete time steps. It\u0026rsquo;s defined by the 5-tuple $(S, A, P, R, \\gamma)$ where:\n$S$ = states $A$ = actions $P(s\u0026rsquo;|s,a)$ = transition probability of reaching $s\u0026rsquo;$ after taking action $a$ in state $s$ $R(s,a,s\u0026rsquo;)$ = reward for this transition $\\gamma \\in [0,1)$ = discount factor for future rewards The Markov Property The future is conditionally independent of the past, given the present state.\nFormally: $$P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)$$\nWhy MDPs Matter They formalize sequential decision making: robotics, games, recommendation systems. They give a clear mathematical foundation for defining and solving RL problems via value functions, policy search, and dynamic programming. How do Markov Decision Processes differ from Hidden Markov Models HMMs deal with partial observability (you only see observations, not the actual underlying states). The context window for HMMs is technically just 1 since they\u0026rsquo;re first-order Markov models, meaning they depend only on the current hidden state. HMM vs MDPs Aspect HMM MDP State Hidden (unobserved) Fully observed Observations Emitted from hidden states Not applicable (agent directly observes state) Actions No actions; just a generative sequence model Agent chooses actions $a$ in each state $s$ Transition Model $P(h_{t+1} \\mid h_t)$ $P(s_{t+1} \\mid s_t,a_t)$ Emission/Reward Emission: $P(o_t \\mid h_t)$ Reward: $R(s_t,a_t,s_{t+1})$ Objective Compute likelihood or decode hidden path Maximize expected cumulative reward Algorithms Forward/backward; Viterbi; Baum–Welch (EM) Value iteration; policy iteration; Q-learning; policy gradients Use Cases Sequence labeling (speech, POS, bioinfo) Sequential decision-making (robotics, games, control) Reward Functions Definition A reward function tells you how \u0026ldquo;good\u0026rdquo; a single transition is. Formally:\n$$R(s,a,s\u0026rsquo;) = \\text{expected immediate reward when you take action } a \\text{ in state } s \\text{ and end up in } s\u0026rsquo;$$\nSymbol Meaning $s$ Current state $a$ Action taken $s'$ Next state $R(s,a,s\u0026rsquo;)$ Reward you get for that $(s \\to s\u0026rsquo;)$ transition Intuition In a game, you might get +10 points for eating a pellet or –100 if you hit a ghost. In finance, you might receive +$5 if a trade succeeds or –$2 if it fails. In fraud detection, you might get +$10 if you correctly detect fraud, -$100 if you miss fraud, -$20 for a false positive, and +$5 if you correctly detect non-fraud. State-Value Function $V^\\pi(s)$ Under a given policy $\\pi$, the value of a state $s$ is the expected, discounted sum of future rewards when you start in $s$ and follow $\\pi$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\mid S_0=s\\right]$$\n$\\gamma$ trades off immediate vs. long-term reward. High $V^\\pi(s)$ means \u0026ldquo;good to be here under $\\pi$\u0026rdquo; $r_{t+1} = R(S_t, A_t, S_{t+1})$ $\\mathbb{E}$ = average over all possible futures under $\\pi$ Action Value Function $Q^\\pi(s,a)$ $Q^\\pi(s, a)$ is the expected (average) discounted return if you take action $a$ in state $s$ now and then follow the policy $\\pi$ afterwards.\nWhy do we need the Action-Value function $Q(s,a)$? Short answer:\nBecause if you only know how good a state is (that\u0026rsquo;s $V(s)$), you still don\u0026rsquo;t know which action to take from that state without either (a) a model of the world to look ahead, or (b) evaluating every action by trial. $Q(s,a)$ tells you the expected return if you take action $a$ now in state $s$, then behave well after. That lets you pick the best action locally without a model.\nIntuition (intersection analogy) You\u0026rsquo;re at a road intersection (state $s$). Knowing \u0026ldquo;this intersection is promising\u0026rdquo; (high $V(s)$) doesn\u0026rsquo;t tell you whether to turn left or right. The thing you actually need at the decision point is: \u0026ldquo;If I turn left right now, how good is that?\u0026rdquo; That number is $Q(s,\\text{left})$. Likewise for right.\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nAction-Value as an Expectation (from returns to Bellman) TL;DR:\nWe turn the scary infinite return into a local recursion: \u0026ldquo;reward now + discounted value later,\u0026rdquo; averaged over what can happen next. This uses two facts:\nthe law of total expectation the Markov property Step 1: Start from the definition (returns view)\n$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k} \\mid S_t=s, A_t=a\\right]$$\nStep 2: Peel off one step\nSeparate the immediate reward from everything after:\n$$Q^\\pi(s,a) = \\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\right]$$\nwhere $G_{t+1} = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+2+k}$ is \u0026ldquo;the return starting next step.\u0026rdquo;\nStep 3: Condition on the next state $s\u0026rsquo;$ (law of total expectation)\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\left[\\mathbb{E}\\left[r_{t+1} + \\gamma G_{t+1} \\mid S_t=s,A_t=a,S_{t+1}=s\u0026rsquo;\\right]\\right]$$\nStep 4: Use the Markov property (make it local)\nGiven $(s,a,s\u0026rsquo;)$:\nImmediate reward depends only on that transition: $\\mathbb{E}[r_{t+1}|s,a,s\u0026rsquo;] = R(s,a,s\u0026rsquo;)$ The future return depends only on the next state (and then following $\\pi$): $\\mathbb{E}[G_{t+1}|S_{t+1}=s\u0026rsquo;] = V^\\pi(s\u0026rsquo;)$ So the inner expectation becomes $R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)$, yielding:\n$$Q^\\pi(s,a) = \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)}\\big[R(s,a,s\u0026rsquo;) + \\gamma V^\\pi(s\u0026rsquo;)\\big]$$\nStep 5: Expand $V^\\pi$ over next actions\nBy definition of the state value: $V^\\pi(s\u0026rsquo;) = \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)$\nPutting it all together gives the Bellman expectation equation for $Q^\\pi$:\n$$\\boxed{Q^\\pi(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;) Q^\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big]}$$\nSimple toy example From state $s$, action $a$ leads to:\n$s_1$ with prob 0.7, reward 5 $s_2$ with prob 0.3, reward 0 Let $\\gamma=0.9$, and suppose $V^\\pi(s_1)=10$ and $V^\\pi(s_2)=2$. Then:\n$$\\begin{align} Q^\\pi(s,a) \u0026amp;= 0.7(5 + 0.9 \\times 10) + 0.3(0 + 0.9 \\times 2) \\ \u0026amp;= 0.7(14) + 0.3(1.8) \\ \u0026amp;= 9.8 + 0.54 \\ \u0026amp;= \\mathbf{10.34} \\end{align}$$\nIt\u0026rsquo;s a weighted average of \u0026ldquo;reward now + discounted value later\u0026rdquo; over possible next states.\nQ-Learning Knowing state value is great—if you also have a model of the world to look ahead. But when you don\u0026rsquo;t, you want to know directly: How good is taking action a in state s right now? That\u0026rsquo;s the action-value ($Q(s,a)$), and Q-learning learns it from experience with no model required.\nTabular Q-Learning — from optimality to an update you can code 1) Objective: optimal action-values $$Q^(s,a) = \\sum_{s\u0026rsquo;} P(s\u0026rsquo;|s,a)\\Big[R(s,a,s\u0026rsquo;) + \\gamma \\max_{a\u0026rsquo;} Q^(s\u0026rsquo;,a\u0026rsquo;)\\Big]$$\nThis is the Bellman optimality equation. If we had $Q^$, the greedy policy $\\pi^(s) = \\arg\\max_a Q^*(s,a)$ is optimal.\n2) One-step TD target (what we aim at each step) Replace the expectation over $s\u0026rsquo;$ with a sample from the environment:\n$$y_{\\text{target}} = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;)$$\nThis is a sampled estimate of the right-hand side of Bellman optimality.\n3) Q-learning update (move toward the target) $$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q(s,a)\\Big]$$\n$\\alpha$ is the learning rate We bootstrap using our own $Q$ on $s'$ Using $\\max_{a\u0026rsquo;}$ (rather than the next action actually taken) makes Q-learning off-policy: it learns about the greedy policy even if behavior is exploratory 4) Exploration: ε-greedy behavior policy We still need to visit state-actions to learn them:\nWith probability ε: take a random action (explore) Otherwise: take $\\arg\\max_a Q(s,a)$ (exploit) Typical schedule: start ε at 1.0, decay to 0.1 (or 0.01) over many episodes.\n5) Tiny numeric example (feel the TD step) Suppose at $(s,a)$ you observe:\nreward $r=1.0$, next state $s'$ current $Q(s,a)=0.50$ $\\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 0.80$ $\\gamma=0.9$, $\\alpha=0.5$ Target: $y = r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) = 1.0 + 0.9 \\times 0.80 = 1.72$\nUpdate: $Q_{\\text{new}}(s,a) = 0.50 + 0.5 \\times (1.72 - 0.50) = \\mathbf{1.11}$\nYou\u0026rsquo;ve moved halfway toward the target.\n6) SARSA vs Expected SARSA vs Q-learning (quick contrast) SARSA (on-policy) uses the actual next action $a\u0026rsquo; \\sim \\pi$: $$Q \\leftarrow Q + \\alpha[r + \\gamma Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Expected SARSA computes the average over next actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ Q-learning (off-policy) uses the max over actions: $$Q \\leftarrow Q + \\alpha[r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;,a\u0026rsquo;) - Q]$$ 7) Minimal NumPy implementation (FrozenLake, tabular) import numpy as np, gym env = gym.make(\u0026#39;FrozenLake-v1\u0026#39;, is_slippery=False) nS, nA = env.observation_space.n, env.action_space.n Q = np.zeros((nS, nA)) alpha, gamma = 0.8, 0.9 eps, eps_min, eps_decay = 1.0, 0.1, 0.995 episodes = 2000 def eps_greedy(s): if np.random.rand() \u0026lt; eps: return env.action_space.sample() return np.argmax(Q[s]) for ep in range(episodes): s, done = env.reset(), False while not done: a = eps_greedy(s) s2, r, done, _ = env.step(a) target = r + (0 if done else gamma * np.max(Q[s2])) Q[s, a] += alpha * (target - Q[s, a]) s = s2 eps = max(eps_min, eps * eps_decay) print(\u0026#34;Q-table:\\n\u0026#34;, Q) Terminal states: if done, set the bootstrapped term to 0 (as above).\nDeep Q-Learning Q-learning works only to an extent where it is possible to keep track of states and actions at a given time. However, as we move away from simple games like tic-tac-toe to something as complex as Chess, the number of states and actions at a given time increases exponentially. At that stage, keeping track of a Q-table becomes very compute intensive, doesn\u0026rsquo;t scale well, and is very slow. Hence, the paradigm of Deep Q-learning provides the potential to overcome this barrier.\nTabular methods break when the state space is huge. DQN replaces the table with a neural net $Q_\\theta(s,a)$ that outputs all action values from a state.\nCore ideas (why DQN works) Function approximation: $Q_\\theta(s,a)$ via a neural net Experience replay: Store $(s,a,r,s\u0026rsquo;,\\text{done})$ transitions, train on random mini‑batches to break correlation Loss and targets For a batch $\\mathcal{B}$ of transitions:\n$$y_i = r_i \\quad \\text{if episode is done}$$\n$$y_i = r_i + \\gamma \\max_{a\u0026rsquo;} Q_{\\theta^-}(s\u0026rsquo;_i, a\u0026rsquo;) \\quad \\text{otherwise}$$\nSquared loss (often Huber in practice):\n$$\\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}} \\big( y_i - Q_\\theta(s_i, a_i) \\big)^2$$\nGradient step: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}(\\theta)$\nTarget network update (periodic hard update every $C$ steps): $\\theta^- \\leftarrow \\theta$\n(or soft update: $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$)\nDouble DQN (reduces overestimation bias) Use the online net to pick the action and the target net to evaluate it:\n$$y_i = r_i + \\gamma Q_{\\theta^-}(s\u0026rsquo;_i, a^*)$$\nwhere $a^* = \\text{argmax}_{a\u0026rsquo;} Q_\\theta(s\u0026rsquo;_i,a\u0026rsquo;)$\nCode Example for Deep Q-Learning For a complete implementation example, check out this GitHub gist with a working DQN on CartPole.\nWhat\u0026rsquo;s Next? In the next post, we\u0026rsquo;ll dive deeper into:\nPolicy Gradient methods (REINFORCE, Actor-Critic) Advanced DQN variants (Dueling DQN, Prioritized Experience Replay) Real-world applications in fraud detection and recommendation systems Stay tuned for more hands-on RL content! 🚀\nHave questions or thoughts about this post? Feel free to reach out - I\u0026rsquo;d love to discuss RL concepts and applications!\n","permalink":"http://localhost:1313/posts/reinforcement-learning-foundations/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) has exploded in popularity — first in game-playing agents, and now in large language models via methods like RLHF (Reinforcement Learning from Human Feedback). These approaches don\u0026rsquo;t just help models learn context better; they also improve reasoning by teaching them to \u0026ldquo;think in steps.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eMy fascination with RL began when the GPT-3 paper was published and ChatGPT emerged as the so-called \u0026ldquo;tool of the decade.\u0026rdquo; I wanted to go beyond using these models — I wanted to understand \u003cem\u003ehow\u003c/em\u003e they work under the hood. That meant building RL concepts from the ground up: deriving equations, implementing toy solutions in environments like CartPole and FrozenLake, and seeing theory come alive in code.\u003c/p\u003e","title":"Reinforcement Learning Foundations: From MDPs to Deep Q-Learning"}]